Project Report: Sentiment and Grade Prediction from Professor Reviews (Neural Network)

---

Introduction:

**What problem are you solving?**
This project aims to predict student grade outcomes and analyze sentiment from professor/course reviews. The goal is to use natural language processing and machine learning to extract sentiment from review text and use it, along with other features, to predict the grades students are likely to receive in a course.

**How do you plan to solve it?**
We use a supervised learning approach. First, we preprocess and clean the review data, then train a sentiment analysis model to classify review sentiment. The sentiment scores, along with other features (e.g., rating, difficulty, professor-level stats), are used to train a Neural Network regression model to predict grades. The pipeline includes data collection, preprocessing, sentiment analysis, grade prediction, and visualization.

**How does this approach relate to the lectures/papers we discussed?**
This approach leverages concepts from supervised learning, text preprocessing, feature engineering, and model evaluation, as discussed in class. It also draws on literature about using sentiment analysis and regression for educational data mining.

---

Motivation:

**Why is your project important? Why are you excited about it?**
Understanding how student perceptions (sentiment) and course/professor features relate to grade outcomes can help students make informed course choices and help institutions improve teaching. The project is exciting because it combines NLP, machine learning, and real-world educational data.

**What are some existing questions in the area?**
- How do subjective student reviews relate to objective outcomes like grades?
- Can sentiment analysis improve grade prediction models?
- What features are most predictive of student success?

**Are there any prior related works? Provide a brief summary.**
Prior work includes educational data mining using regression and classification to predict grades, and sentiment analysis of course reviews to understand student satisfaction. This project extends these by combining both approaches.

---

Method:

**What dataset did you use?**
The dataset consists of professor/course reviews scraped from RateMyProfessors and similar sources, stored in JSON and CSV files. The data includes review text, ratings, difficulty, grades, professor names, departments, and other metadata.

**What form does this data have? Is it images, raw text, tabular, etc? What are the features?**
The data is tabular, with both structured (numerical/categorical) and unstructured (text) features. Key features:
- rating (numerical)
- difficulty (numerical)
- grade (categorical/converted to numerical)
- text (raw review text)
- cleaned_text (preprocessed text)
- professor_name, department, etc.

**What kind of model did you use?**
- Sentiment Analysis: Naive Bayes classifier on TF-IDF features from cleaned review text.
- Grade Prediction: Neural Network Regressor (MLPRegressor) using rating, difficulty, sentiment score, and professor-level features.

**How did you define the problem/feature space?**
- Sentiment is defined as binary (positive/negative) based on rating threshold.
- Grade prediction uses both review features and aggregated professor stats.

**What would be your implementation steps? How will you evaluate your method?**
1. Data collection and splitting into train/test sets.
2. Preprocessing: cleaning text, handling missing values, mapping grades.
3. Sentiment model training and evaluation (accuracy, F1, classification report).
4. Adding sentiment scores to data.
5. Grade prediction model training and evaluation (RMSE, MAE, R2).
6. Visualization of results and feature importance.

**How will you test and measure success?**
- Sentiment: Accuracy, F1 score, classification report on test set.
- Grade prediction: RMSE, MAE, R2 on test set. Feature importance analysis.

---

Results:

**How did your model perform?**
- Sentiment Analysis (Naive Bayes):
  - Training Accuracy: 0.7370, F1 Score: 0.8446
  - Test Accuracy: 0.8000, F1 Score: 0.8880
  - Classification report shows high recall for positive sentiment, lower for negative (class imbalance).
- Grade Prediction (Neural Network):
  - Training RMSE: 0.232
  - Test RMSE: 0.280
  - Test MAE: 0.178
  - Test R2: -0.266 (model explains little variance on test set)

**Feature Importance (from Neural Network):**
- rating: 0.00
- difficulty: 0.00
- sentiment_score: 0.00
- review_count: 0.00
- avg_rating: 0.00
- avg_difficulty: 0.00

*Note: MLPRegressor does not provide feature importances; zeros are returned.*

**Visualizations:**
- Sentiment distribution: Shows the spread of predicted sentiment scores.
- Grade correlation: Scatterplot of sentiment score vs. grade.
- Difficulty vs. rating: Relationship between perceived difficulty and rating.
- Department comparison: Boxplot of ratings by department.
- Feature importance: Barplot of model feature importances (all zero for NN).
- Correlation matrices: Heatmaps of feature correlations.

---

Discussion:

**What outcome did you expect from your results?**
Expected that sentiment and ratings would be strong predictors of grades, and that the model would generalize well to test data.

**How did your actual results differ from your expected results?**
- Sentiment model performed well, but grade prediction model had low R2 on test set, indicating limited generalization.
- Feature importance is not available for the neural network model, making interpretation difficult.
- Visualizations confirm relationships but also show noise and variability in the data.

**How does this method compare to existing methods?**
- Neural Network performed slightly worse than Random Forest and Gradient Boosting on this dataset, with the lowest R2 and highest test RMSE.

**What new questions do these results raise, and how can they be addressed by further analysis?**
- Can we improve neural network performance with more data or hyperparameter tuning?
- Are there alternative ways to interpret feature importance for neural networks (e.g., SHAP, permutation importance)?
- How does the lack of interpretability affect the usefulness of the model?

---

(End of Report) 